---
layout:     post
title:      手把手带你搞定直播(三)
subtitle:   FFmpeg从零搭建直播APP
date:       2020-08-23
author:     MLX
header-img: img/post-bg-2020-08-22.jpg
catalog: 	 true
tags:
    - Android
    - FFmpeg
    - 音视频

---

## 前言

上一篇文章中，我们一起学习了音视频的基础知识，这一节我们将开发一个用于播放本地或者网络流的播放器。话不多说开始~

## 流程图

首先从上一节基础知识中已经了解到，视频的封装格式很多，所以我们拿到一个视频以后首先第一步就是解封装，然后拿到了视频流和音频流，这里的视频流和音频流是编码后的格式，我们如果要播放的话就需要解码。解码以后拿到了每一帧的数据就可以播放了。

这是流程图可以更好的理解：

<img src="/img/FFmpeg解封装流程.png" style="zoom:67%;" />

## 架构图

流程图大家都是应该了解了，可是FFmpeg很多小伙伴从来没接触过，根本不知道如果解封装，也不知道如何解码，其实就是一些API的调用，为了方便大家理解，我这里绘制了带API的播放器的架构图。详细的描述了每一步的步骤：

![](/img/ffplayer.png)

可能有的小伙伴看了这个图已经开始骂人了，卧槽这么复杂，学不下去了！再见！

兄dei，莫急，听我慢慢道来。

图看起来确实比较复杂，但是聪明的小伙伴已经能明白其中的意思了。emmm，不明白的也不是说不聪明，可能是没接触过这样的方式。首先左边就是解封装的流程，对应的是流程图中解封装步骤，右边两个分别是视频和音频的解码和播放步骤。细心的小伙伴已经发现了，视频和音频的处理方式几乎是一毛一样的~唯一的不同就是播放方式和视频部分多了一小节。这一小节就是处理音视频同步的，在后面会讲~

我先来给大家分析一下

首先第一步就是先拿到媒体文件，甭管它是本地文件还是网络流，我们要打开它，然后拿到其中的视频流/音频流信息，然后根据它编码方式查找到对应的解码器，设置一些参数balabalabala。最后打开解码器，从视频流/音频流中一帧一帧的读取数据包。由于这是个循环，并且是个耗时操作，我们需要开启一个线程去处理它。

当我们拿到数据包以后，判断它是视频包还是音频包，并把它交给视频类的数据包队列或音频类的数据包队列中去处理可以看到不管是视频类还是音频类都有这个队列。

以视频为例，视频类同样需要开启一个新的线程循环的从队列中读取数据包，然后根据解码器去解码成原始数据。并将原始数据添加到原始数据的的队列中。视频类另外再开启一个渲染的线程，用以循环不断地从原始数据队列中取出原始数据YUV格式的图片，并将YUV格式的图片转换为RGB图像，将其渲染到屏幕上。音视频同步在后面统一讲，这里先不做叙述。

这样讲的话小伙伴们应该大致了解了。下面开始代码走起~

## 解封装

首先定义一个Player的C++类，进行一个初始化解码器的操作。

```c++
void MlxPlayer::_prepare() {
    //AVDictionary是一个健值对存储工具，类似于c++中的map
    AVDictionary *dictionary = 0;
    //设置读取超时
    av_dict_set(&dictionary, "timeout", "10000000", 0);
    //打开媒体文件，这里的dataSource就是Kotlin传来的本地文件地址或者网络地址，formatContext是全局的上下文参数，如果比喻的话可以类比Activity中的Context
    int openStatus = avformat_open_input(&formatContext, this->dataSource, 0, &dictionary);
    //如果打开状态失败就回调Koltin接口，提示失败信息
    if (openStatus) {
        this->javaCallBack->onPerpareFaild(THREAD_CHILD, openStatus);
        return;
    }
    //通过全局上下文参数获取媒体文件中的流信息
    int findStreamCode = avformat_find_stream_info(formatContext, 0);
    //如果没有找到流信息，依然回调Kotlin失败接口
    if (findStreamCode) {
        this->javaCallBack->onPerpareFaild(THREAD_CHILD, findStreamCode);
        return;
    }
	//里面可能不止一个流信息，可能有视频流，音频流，甚至是字幕流。所以遍历所有的流
    for (int i = 0; i < formatContext->nb_streams; ++i) {
        //获取流
        AVStream *avStream = formatContext->streams[i];
        //获取流的编码参数
        AVCodecParameters *parameters = avStream->codecpar;
        //通过id查找解码器
        AVCodec *avCodec = avcodec_find_decoder(parameters->codec_id);
        //创建解码器上下文
        AVCodecContext *codecContext = avcodec_alloc_context3(avCodec);
        //设置编码参数
        avcodec_parameters_to_context(codecContext, parameters);
        //打开解码器
        int ret = avcodec_open2(codecContext, avCodec, 0);
        //失败则回调
        if (ret) {
            this->javaCallBack->onPerpareFaild(THREAD_CHILD, -12);
            return;
        }
        //如果是视频，则创建视频类，否则是音频类。并传递流的索引和解码器上下文
        if (parameters->codec_type == AVMEDIA_TYPE_VIDEO) {
            videoChannle = new VideoChannle(i, codecContext);
        } else if (parameters->codec_type == AVMEDIA_TYPE_AUDIO) {
            audioChannle = new AudioChannle(i, codecContext);
        }
    }
    //如果这里都没有的话代表既没有视频也没有音频，回调失败接口
    if (!audioChannle && !videoChannle) {
        this->javaCallBack->onPerpareFaild(THREAD_CHILD, -10);
        return;
    }
    //回调成功接口，表示下一步可以开始解码并播放了。
    this->javaCallBack->onPerpareSuccess(THREAD_CHILD);
}
```

`javaCallBack`是一个回调的类，主要负责C++调用Java/Kotlin,并将各种结果传递给上层。

其他的说明在上述的注释里已经讲得很详细了。

## 从流中读取数据包

```c++
void MlxPlayer::_start() {   
    //定义一个入参AVPacket，用于接收数据包
    AVPacket *packet;
    //循环读取
    while (isPlaying) {
        if(!isPlaying){
            break;
        }
        if (videoChannle && videoChannle->packets.size() > 20) {
            av_usleep(20 * 1000);
            continue;
        }
        if (audioChannle && audioChannle->packets.size() > 20) {
            av_usleep(20 * 1000);
            continue;
        }
        if(!isPlaying){
            break;
        }
        //申请内存
        packet = av_packet_alloc();
        //锁定当前线程
        pthread_mutex_lock(&mutex);
        //读取一个数据包
        int ret = av_read_frame(formatContext, packet);
        //解锁当前线程
        pthread_mutex_unlock(&mutex);
        //如果读取到了，看源代码的话，可以了解到如果成功了，ret就是0，否则就是其他数值。
        if (!ret) {
            //如果存在video类，并且包的流索引于视频索引一样就代表是视频包，则将其添加到视频队列中，下面音频同理
            if (videoChannle && packet->stream_index == videoChannle->id) {
                videoChannle->packets.push(packet);
            } else if (audioChannle && packet->stream_index == audioChannle->id) {
                audioChannle->packets.push(packet);
            }
            
        } else if (ret == AVERROR_EOF) {//这里是读取到了文件末尾
            //如果视频/音频中数据包和原始数据包的队列都为空了，那么我们认为是播放结束了。
            if (videoChannle->packets.empty() && videoChannle->frames.empty()
                && audioChannle->packets.empty() && audioChannle->frames.empty()) {
                //停止播放并且回调播放结束的接口
                isPlaying = 0;
                if (javaCallBack) {
                    this->javaCallBack->onPlayEnd();
                }
                break;
            }
        } else {//其他错误的话就回调错误接口即可
            if (javaCallBack) {
                this->javaCallBack->onPerpareFaild(THREAD_CHILD, ret);
            }
            isPlaying = 0;
            break;
        }
    }
    //最后释放这个包所占用的内存空间
    av_packet_free(&packet);
}
```

该有的注释上面基本都有了，需要说明的是上面出现了两次`isPlaying`为空的判定，这是因为`av_usleep(20 * 1000);`是一个线程休眠的方法，在这段休眠过程中很可能其他线程操作了`isPlaying`已经停止播放了，所以需要进行二次判定。理论上来说所有的耗时方法结束以后都应该进行一次判定，但是后面有其他方式去解决这个问题。

还有一点就是下面的这段代码：

```c++
if (videoChannle && videoChannle->packets.size() > 20) {
         av_usleep(20 * 1000);
         continue;
}
if (audioChannle && audioChannle->packets.size() > 20) {
        av_usleep(20 * 1000);
        continue;
}
```

其实这个也很好理解，就是控制队列中的数据容量的，有可能播放很慢，但是读取数据很快，导致队列中的数据越攒越多，出现内存泄漏的问题。就是一个消费者生产者得问题，消费很慢，生产很快。这里就控制不让生产，先消费一段时间，再继续恢复生产。

## 视频解码

首先是创建一个解码线程和一个用于播放的线程

```c++
void VideoChannel::start() {
    isPlaying = 1;
    pthread_create(&decode_thread, 0, _decode, this);
    pthread_create(&play_thread, 0, _play, this);
}
```

其中_decode方法是在子线程中运行的，然后通过传过去的this参数强转为VideoChannel,调用其decode方法完成真正的解码：

```c++
void *_decode(void *args) {
    auto *channel = static_cast<VideoChannel *>(args);
    channel->decode();
    return 0;
}
```

```c++
void VideoChannel::decode() {
    AVPacket *packet = 0;
    while (isPlaying) {
        //依然是控制队列容量，避免内存泄漏现象
        if (frames.size() > 20) {
            continue;
        }
        int ret = packets.pop(packet);
        //没有取到数据包就进行下一次循环
        if (!ret) {
            continue;
        }
        //发送编码数据包
        ret = avcodec_send_packet(codeContext, packet);
        if (ret) {
            break;
        }
		//定义一个原始数据包并申请内存
        AVFrame *frame = av_frame_alloc();
        //接收解码后的原始数据
        ret = avcodec_receive_frame(codeContext, frame);
        releaseAVPacket(&packet);//释放packet，后面不需要了
        if (ret == AVERROR(EAGAIN)) {
            continue;
        } else if (ret != 0) {
            releaseAVPacket(&packet);
            break;
        }
        //将原始数据添加到原始数据队列，这里面的原始数据就是yuv图像
        frames.push(frame);
    }
    releaseAVPacket(&packet);
}
```

解码的具体方法就是上面的示例，大概意思相信小伙伴一眼就能看得懂。下面就是播放的方法：

```c++
void VideoChannel::play() {
    AVFrame *frame = 0;
    int ret = 0;
    //在这里写是为了方便大家看到，后续应该写到构造方法中去
    swsContext =sws_getContext(codeContext->width, codeContext->height,codeContext->pix_fmt, codeContext->width, codeContext->height,AV_PIX_FMT_RGBA,SWS_BILINEAR, NULL, NULL, NULL);
    //同上
    av_image_alloc(dst_data, dst_line, codeContext->width, codeContext->height,
                   AV_PIX_FMT_RGBA, 1);
    while (isPlaying) {
        if (!isPlaying) {
            break;
        }    
        //取出原始数据包
        ret = frames.pop(frame);  
        //如果没有就继续
        if (!ret) {
            continue;
        }
       
        sws_scale(swsContext, frame->data, frame->linesize, 0, codeContext->height, dst_data,dst_line);
        callback(dst_data[0], dst_line[0], codeContext->width, codeContext->height);
        releaseAVFrame(&frame);
    }
    releaseAVFrame(&frame);
}
```

libswscale是一个主要用于处理图片像素数据的类库。可以完成图片像素格式的转换，图片的拉伸等工作。其优势在于：可以在同一个函数里实现：1.图像色彩空间转换， 2:分辨率缩放，3:前后图像滤波处理。不足之处在于：效率相对较低，不如libyuv或shader。

sws_getContext是像素格式转换的上下文，会创建新的空间。下面是关于sws_getContext代码参数的说明：

```c++
truct SwsContext *sws_getContext(
            int srcW, /* 输入图像的宽度 */
            int srcH, /* 输入图像的宽度 */
            enum AVPixelFormat srcFormat, /* 输入图像的像素格式 */
            int dstW, /* 输出图像的宽度 */
            int dstH, /* 输出图像的高度 */
            enum AVPixelFormat dstFormat, /* 输出图像的像素格式 */
            int flags,/* 选择缩放算法(只有当输入输出图像大小不同时有效),一般选择SWS_FAST_BILINEAR */
            SwsFilter *srcFilter, /* 输入图像的滤波器信息, 若不需要传NULL */
            SwsFilter *dstFilter, /* 输出图像的滤波器信息, 若不需要传NULL */
            const double *param /* 特定缩放算法需要的参数(?)，默认为NULL */
            );
```

sws_scale是真正用于转换的方法，这个是它的定义：

```c++
int sws_scale(struct SwsContext *c, const uint8_t *const srcSlice[],
              const int srcStride[], int srcSliceY, int srcSliceH,
              uint8_t *const dst[], const int dstStride[]);
```

1.参数 `SwsContext *c`， **转换格式的上下文**。也就是 `sws_getContext` 函数返回的结果。

2.参数 `const uint8_t *const srcSlice[]`, **输入图像的每个颜色通道的数据指针**。其实就是解码后的AVFrame中的`data[]`数组。因为不同像素的存储格式不同，所以`srcSlice[]`维数也有可能不同。
以YUV420P为例，它是planar格式，它的内存中的排布如下：
YYYYYYYY UUUU VVVV
使用FFmpeg解码后存储在AVFrame的data[]数组中时：
`data[0]`——-Y分量, Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8……
`data[1]`——-U分量, U1, U2, U3, U4……
`data[2]`——-V分量, V1, V2, V3, V4……
linesize[]数组中保存的是对应通道的数据宽度 ，
`linesize[0]`——-Y分量的宽度
`linesize[1]`——-U分量的宽度
`linesize[2]`——-V分量的宽度

而RGB24，它是packed格式，它在`data[]`数组中则只有一维，它在存储方式如下：
`data[0]: R1, G1, B1, R2, G2, B2, R3, G3, B3, R4, G4, B4……`
这里要特别注意，`linesize[0]`的值并不一定等于图片的宽度，有时候为了对齐各解码器的CPU，实际尺寸会大于图片的宽度。

3.参数`const int srcStride[]`，**输入图像的每个颜色通道的跨度**。.也就是每个通道的行字节数，对应的是解码后的AVFrame中的`linesize[]`数组。根据它可以确立下一行的起始位置，不过stride和width不一定相同，这是因为：
*a.由于数据帧存储的对齐，有可能会向每行后面增加一些填充字节这样 `stride = width + N`；*
*b.packet色彩空间下，每个像素几个通道数据混合在一起，例如RGB24，每个像素3字节连续存放，因此下一行的位置需要跳过3width字节。*

> 拿到视频的帧数据后，可能width并不等于linesize，直接使用width来生成纹理会导致扭曲。
>
> width是视频的宽度，这个是实际的，`linesize`是一行数据的字节数量。图像是2维的，但数据存储确实1维的，即`uint8_t*`，而不是用二维数组来存储,不是每行数据单独一个指针。
>
> 所以就需要知道一行的数据大小，这样才能确定每一行的数据开头在哪。比如数据是：12345678,每行两个，那第二行就是34,如果每行3个，那么就是456。
>
> 对于YUV420p,3个分量是分层的，但看Y，每个像素的1个字节，如果宽度为100，那么一行就有100个Y,那么linesize就是100。也就是说一般情况下，Y这个层的`linesize`就等于`width`,那么为什么会可能会不等于呢？应该是因为编码的时候使用**内存对齐**导致的。比如视频宽度232,然后使用24作为对齐的基本单位，那么linesize就会变成240，会多出8个字节。
>
> 如果直接使用width作为linesize，为什么会导致扭曲？
>
> 假设正确数据是：
>
> ```undefined
> 1234567x
> 1234567x
> 1234567x
> ```
>
> x是对齐多出来的无用数据，使用linesize为8就是正常显示，然后width是7，使用7就会变成:
>
> ```undefined
> 1234567
> x123456
> 7x12345
> ```
>
> 单看1或者2，就会发现变成倾斜的了，实际看起来视频就是这样充满了右斜的线。

4.参数`int srcSliceY, int srcSliceH`,定义在输入图像上处理区域，`srcSliceY`是**起始位置**，`srcSliceH`是**处理多少行**。如果`srcSliceY=0，srcSliceH=height`，表示一次性处理完整个图像。这种设置是为了多线程并行，例如可以创建两个线程，第一个线程处理 [0, h/2-1]行，第二个线程处理 [h/2, h-1]行。并行处理加快速度。
5.参数`uint8_t *const dst[], const int dstStride[]`定义输出图像信息（输出的每个颜色通道数据指针，每个颜色通道行字节数）

## 渲染

`callback(dst_data[0], dst_line[0], codeContext->width, codeContext->height);`这一行就是把数据和通道数，宽高通过回调传出去，通过Surface渲染。为什么要用Surface呢，因为Surface是在单独的渲染线程中渲染UI，所以很适合这种十分频繁的渲染。首先创建一个于Surface关联的`ANativeWindow`对象，这个`ANativeWindow`对象是C/C++中定义的一个结构体,等同于Java中的`Surface`。`ANativeWindow`中存放像素信息的结构是:`ANativeWindow_Buffer`。其主要API就是：

- 获取与surface对应的ANativeWindow

  ```cpp
  ANativeWindow* ANativeWindow_fromSurface(JNIEnv* env, jobject surface);
  ```

- 保持/释放ANativeWindow对象的引用

  ```cpp
  void ANativeWindow_acquire(ANativeWindow* window);
  
  void ANativeWindow_release(ANativeWindow* window);
  ```

- 向buffer中写入数据并提交

  ```cpp
  int32_t ANativeWindow_lock(ANativeWindow* window, ANativeWindow_Buffer* outBuffer,
          ARect* inOutDirtyBounds);
  
  // 这之间的代码可以执行一些向buffer中写入数据的操作
  
  int32_t ANativeWindow_unlockAndPost(ANativeWindow* window);
  ```

- 获取Window Surface的信息:宽/高/像素格式

  ```cpp
  int32_t ANativeWindow_getWidth(ANativeWindow* window);
  
  int32_t ANativeWindow_getHeight(ANativeWindow* window);
  
  int32_t ANativeWindow_getFormat(ANativeWindow* window);
  ```

  像素格式定义在`AHARDWAREBUFFER_FORMAT_*`

### 渲染过程中一般的流程

1. 通过`ANativeWindow_fromSurface`获取与Surface对应的`ANativeWindow`对象
2. `ANativeWindow_setBuffersGeometry`设置buffer的尺寸和格式
3. `ANativeWindow_acquire`获取引用对象
4. 利用`ANativeWindow_lock/ANativeWindow_unlockAndPost`与之间的绘制代码绘制图像
5. `ANativeWindow_release`释放引用

创建`ANativeWindow`对象：

```c++
extern "C"
JNIEXPORT void JNICALL
Java_com_mlx_mlxplayer_MlxPlayer_nativeSetSurface(JNIEnv *env, jobject thiz, jobject surface) {
    pthread_mutex_lock(&mutex);
    if (window) {
        //如果有的话就就先释放，然后重新创建一个。
        ANativeWindow_release(window);
    }
    window = ANativeWindow_fromSurface(env, surface);
    pthread_mutex_unlock(&mutex);
}
```

这个方法一般是在Surface重新创建的时候才会调用。接下来就是重头戏，真正的渲染数据：

```c++
void renderFrame(uint8_t * dstData,int line,int width,int height){
    if(!player||!player->isPlaying){
        return;
    }
    //锁定线程
    pthread_mutex_lock(&mutex);
    if(!window){
        //window不存在直接解锁线程退出
        pthread_mutex_unlock(&mutex);
        return;
    }
    //设置buffer的尺寸和格式
    ANativeWindow_setBuffersGeometry(window,width,height,WINDOW_FORMAT_RGBA_8888);
    ANativeWindow_Buffer * buffer;
    if(ANativeWindow_lock(window,buffer,0)){
        ANativeWindow_release(window);
        window=0;
        pthread_mutex_unlock(&mutex);
    }
    uint8_t * outData= static_cast<uint8_t *>(buffer->bits);
    //RGBA四个通道，所以要乘以四
    int lineSize=buffer->stride *4 ;
    for (int i = 0; i < height; ++i) {
        //一行一行的内存拷贝
        memcpy(outData + i*lineSize,dstData+i*line,lineSize);
    }
    ANativeWindow_unlockAndPost(window);
    //解锁
    pthread_mutex_unlock(&mutex);
}
```

到这里视频的渲染就结束了。音频的播放和音视频同步将在下一节讲。